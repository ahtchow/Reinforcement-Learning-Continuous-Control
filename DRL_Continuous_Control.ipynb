{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Continous Control\n",
    "---\n",
    "\n",
    "As you've seen, the environment for this project involves controlling a double-jointed arm, to reach target locations. \n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "![reacher](images/reacher.gif)\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1. This is a continous action space, thus we will use some actor-critic method to tackle this environment.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from DDPG_Agent import DDPG_Agent\n",
    "from utils import plot_scores\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DISTRIBUTED_TRAINING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Version (Distributed Training)\n",
    "\n",
    "For this project, we will provide you with two separate versions of the Unity environment:\n",
    "\n",
    "- The first version contains a single agent.\n",
    "- The second version contains 20 identical agents, each with its own copy of the environment.\n",
    "\n",
    "The second version is useful for algorithms like PPO, A3C, and D4PG that use multiple (non-interacting, parallel) copies of the same agent to distribute the task of gathering experience. Since I like a bit of a challenge I will be attempting to solve the second version where 20 agents are sharing a policy.\n",
    "\n",
    "### Solving the Environment\n",
    "\n",
    "**Option 1**: Solve the First Version\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes.\n",
    "\n",
    "**Option 2**: Solve the Second Version\n",
    "\n",
    "The barrier for solving the second version of the environment is slightly different, to take into account the presence of many agents. In particular, your agents must get an average score of +30 (over 100 consecutive episodes, and over all agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 20 (potentially different) scores. We then take the average of these 20 scores.\n",
    "- This yields an average score for each episode (where the average is over all 20 agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISTRIBUTED_TRAINING == True:\n",
    "    # select this option to load version 2 (with 20 agents) of the environment\n",
    "    app_name = \"data/Reacher_20_Agents_Windows_x86_64/Reacher_Windows_x86_64/Reacher.exe\"\n",
    "else:\n",
    "    # select this option to load version 1 (with a single agent) of the environment\n",
    "    app_name = \"data/Reacher_Windows_x86_64/Reacher.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_environment(env, brain):\n",
    "    print(brain)\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space \n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "    print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent: DDPG w/ Ornstien Uhlenbeck Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_goal_ddpg(agent,\n",
    "                    env,\n",
    "                    brain_name,\n",
    "                    n_agents,\n",
    "                    n_episodes=1500,\n",
    "                    max_t=3000,\n",
    "                    print_every=10,\n",
    "                    win_condition=30.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Continious Control using DDPG.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        print_every (int): how many episodes before printing scores\n",
    "        n_agents (int): how many arms are in the environment\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    scores = []\n",
    "    scores_mean = []\n",
    "    scores_window = deque(maxlen=100) # Score last 100 scores\n",
    "    \n",
    "    for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations             # get the current state (for each agent)\n",
    "        score = np.zeros(n_agents)                     # initialize the score (for each agent)\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)                   # consult agent for actions\n",
    "            env_info = env.step(actions)[brain_name]      # take a step in the env\n",
    "            next_states = env_info.vector_observations    # get next state (for each agent)\n",
    "            rewards = env_info.rewards                    # get reward (for each agent)\n",
    "            dones = env_info.local_done                   # see if episode finished\n",
    "            \n",
    "            # take a learning step\n",
    "            agent.step(states, actions, rewards, next_states, dones)  \n",
    "            \n",
    "            score += env_info.rewards                    # update the score (for each agent)\n",
    "            states = next_states                          # roll over states to next time step\n",
    "            if np.any(dones):                             # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        scores.append(np.mean(score))\n",
    "        scores_window.append(np.mean(score))\n",
    "        scores_mean.append(np.mean(scores_window))\n",
    "        \n",
    "        # Print on print_every condition\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tScore: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(score), np.mean(scores_window)))    \n",
    "        \n",
    "        # Winning condition + save model parameters    \n",
    "        if np.mean(scores_window) >= win_condition:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\t Score: {:.2f} \\tAverage Score: {:.2f}'.format(i_episode, np.mean(score), np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "\n",
    "    execution_info = {'last_score': scores.pop(),\n",
    "                      'solved_in': i_episode,\n",
    "                      'last_100_avg': np.mean(scores_window),\n",
    "                      'save_file': 'DDPG/...'}\n",
    "    \n",
    "    return scores, scores_mean, execution_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  7.90150833e+00 -1.00000000e+00\n",
      "  1.25147629e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.22214413e-01]\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=app_name)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Info \n",
    "states = env_info.vector_observations \n",
    "state_size = states.shape[1]\n",
    "action_size = brain.vector_action_space_size, \n",
    "examine_environment(env, brain)\n",
    "n_agents = len(env_info.agents)\n",
    "\n",
    "# Create the agent\n",
    "ddpg = DDPG_Agent(state_size=state_size,\n",
    "                  action_size=action_size[0],\n",
    "                  n_agents = n_agents,\n",
    "                  random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182472e8793549a39053f9a5a47ae18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tScore: 1.44\tAverage Score: 0.91\n",
      "Episode 20\tScore: 2.69\tAverage Score: 1.26\n",
      "Episode 30\tScore: 4.88\tAverage Score: 2.22\n",
      "Episode 40\tScore: 7.05\tAverage Score: 3.35\n",
      "Episode 50\tScore: 9.82\tAverage Score: 4.26\n",
      "Episode 60\tScore: 8.48\tAverage Score: 5.10\n",
      "Episode 70\tScore: 8.30\tAverage Score: 5.69\n",
      "Episode 80\tScore: 9.83\tAverage Score: 6.06\n",
      "Episode 90\tScore: 7.77\tAverage Score: 6.31\n",
      "Episode 100\tScore: 7.03\tAverage Score: 6.40\n",
      "Episode 110\tScore: 8.32\tAverage Score: 7.07\n",
      "Episode 120\tScore: 8.07\tAverage Score: 7.69\n",
      "Episode 130\tScore: 6.47\tAverage Score: 7.97\n",
      "Episode 140\tScore: 6.79\tAverage Score: 8.00\n",
      "Episode 150\tScore: 6.40\tAverage Score: 7.91\n",
      "Episode 160\tScore: 5.78\tAverage Score: 7.67\n",
      "Episode 170\tScore: 4.92\tAverage Score: 7.31\n",
      "Episode 180\tScore: 3.83\tAverage Score: 6.93\n",
      "Episode 190\tScore: 5.28\tAverage Score: 6.61\n",
      "Episode 200\tScore: 5.15\tAverage Score: 6.41\n"
     ]
    }
   ],
   "source": [
    "scores, scores_mean, execution_info =  follow_goal_ddpg(ddpg,\n",
    "                                                        env,\n",
    "                                                        brain_name,\n",
    "                                                        n_agents)\n",
    "\n",
    "# Close environment\n",
    "env.close()\n",
    "\n",
    "# print stats\n",
    "plot_scores(scores, scores_mean, execution_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
